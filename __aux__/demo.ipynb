{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssseb\\.conda\\envs\\Prueba\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ssseb\\.conda\\envs\\Prueba\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ssseb\\.conda\\envs\\Prueba\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import time\n",
    "import kdbai_client as kdbai\n",
    "import pandas as pd\n",
    "import sounddevice as sd\n",
    "import wave\n",
    "import threading\n",
    "import soundfile as sf\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageBindModel(\n",
       "  (modality_preprocessors): ModuleDict(\n",
       "    (vision): RGBDTPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 1280), requires_grad=True)\n",
       "      \n",
       "      (rgbt_stem): PatchEmbedGeneric(\n",
       "        (proj): Sequential(\n",
       "          (0): PadIm2Video()\n",
       "          (1): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 257, 1280), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (text): TextPreprocessor(\n",
       "      (pos_embed): tensor((1, 77, 1024), requires_grad=True)\n",
       "      (mask): tensor((77, 77), requires_grad=False)\n",
       "      \n",
       "      (token_embedding): Embedding(49408, 1024)\n",
       "    )\n",
       "    (audio): AudioPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 768), requires_grad=True)\n",
       "      \n",
       "      (rgbt_stem): PatchEmbedGeneric(\n",
       "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10), bias=False)\n",
       "        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 229, 768), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (depth): RGBDTPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 384), requires_grad=True)\n",
       "      \n",
       "      (depth_stem): PatchEmbedGeneric(\n",
       "        (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "        (norm_layer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 197, 384), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (thermal): ThermalPreprocessor(\n",
       "      (cls_token): tensor((1, 1, 768), requires_grad=True)\n",
       "      \n",
       "      (rgbt_stem): PatchEmbedGeneric(\n",
       "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "        (pos_embed): tensor((1, 197, 768), requires_grad=True)\n",
       "        \n",
       "      )\n",
       "    )\n",
       "    (imu): IMUPreprocessor(\n",
       "      (pos_embed): tensor((1, 251, 512), requires_grad=True)\n",
       "      (cls_token): tensor((1, 1, 512), requires_grad=True)\n",
       "      \n",
       "      (imu_stem): PatchEmbedGeneric(\n",
       "        (proj): Linear(in_features=48, out_features=512, bias=False)\n",
       "        (norm_layer): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (modality_trunks): ModuleDict(\n",
       "    (vision): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (12): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (13): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (14): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (15): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (16): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (17): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (18): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (19): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (20): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (21): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (22): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (23): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (24): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (25): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (26): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (27): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (28): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (29): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (30): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (31): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (text): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (12): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (13): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (14): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (15): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (16): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (17): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (18): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (19): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (20): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (21): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (22): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (23): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (audio): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.009)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.018)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.027)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.036)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.045)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.055)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.064)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.082)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (depth): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (thermal): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "    (imu): SimpleTransformer(\n",
       "      (pre_transformer_layer): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): EinOpsRearrange()\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.140)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.280)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.420)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.560)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BlockWithMasking(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.700)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (post_transformer_layer): EinOpsRearrange()\n",
       "    )\n",
       "  )\n",
       "  (modality_heads): ModuleDict(\n",
       "    (vision): Sequential(\n",
       "      (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=1280, out_features=1024, bias=False)\n",
       "    )\n",
       "    (text): SelectEOSAndProject(\n",
       "      (proj): Sequential(\n",
       "        (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (audio): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=768, out_features=1024, bias=False)\n",
       "    )\n",
       "    (depth): Sequential(\n",
       "      (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=384, out_features=1024, bias=False)\n",
       "    )\n",
       "    (thermal): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Linear(in_features=768, out_features=1024, bias=False)\n",
       "    )\n",
       "    (imu): Sequential(\n",
       "      (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): SelectElement()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (modality_postprocessors): ModuleDict(\n",
       "    (vision): Normalize()\n",
       "    (text): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n",
       "    )\n",
       "    (audio): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=20.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "    (depth): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "    (thermal): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=10.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "    (imu): Sequential(\n",
       "      (0): Normalize()\n",
       "      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logeo en KDBAI_CLOUD\n",
    "KDBAI_API_KEY = '23ee789f8a-fzddBKVhzQPI0ibTnosO0E/LmSr8Q+zPW5AR8vVXrVzKfVyU+Q26BG1tPwyD1vBE9MXhimPI/0m2bZ1h'\n",
    "KDBAI_ENDPOINT = 'https://cloud.kdb.ai/instance/xtg7rzi99h'\n",
    "session = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n",
    "db = session.database('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuración de la tabla.\n",
    "# Configuración del esquema de la tabla\n",
    "schema = [\n",
    "    {\"name\": \"id\", \"type\": \"str\"},\n",
    "    {\"name\": \"embeddings\", \"type\": \"float32s\"}\n",
    "]\n",
    "\n",
    "# Definición del índice\n",
    "index = [\n",
    "    {\n",
    "        \"name\": \"flat_index\",\n",
    "        \"type\": \"flat\",\n",
    "        \"column\": \"embeddings\",\n",
    "        \"params\": {\"dims\": 1024, \"metric\": \"CS\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "# Creación de la tabla\n",
    "table = 'tb_audio'\n",
    "try:\n",
    "    db.table(table).drop()\n",
    "except kdbai.KDBAIException:\n",
    "    pass\n",
    "table_audio = db.create_table(table, schema=schema, indexes=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerarEmbeddings(audio_path):\n",
    "    audio_data = data.load_and_transform_audio_data([audio_path], device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model({ModalityType.AUDIO: audio_data})\n",
    "\n",
    "    audio_embeddings = embeddings[ModalityType.AUDIO].cpu().numpy().flatten()\n",
    "\n",
    "    return audio_embeddings\n",
    "\n",
    "#Prepara una lista de embeddings para un label específico.\n",
    "#Sirve para cargar la base de datos específica de un label.\n",
    "def PrepararEmbedding(label, list_emb):\n",
    "    dict_emb = {\"id\": [label for i in list_emb], 'embeddings': [i for i in list_emb]}\n",
    "    df = pd.DataFrame(dict_emb)\n",
    "    return df\n",
    "\n",
    "def InsertarAudios(path, label):\n",
    "    if os.path.exists(path):\n",
    "      audios_wav = glob.glob(os.path.join(path, '*'))\n",
    "      if len(audios_wav) > 0:\n",
    "        list_emb = [GenerarEmbeddings(i) for i in audios_wav]\n",
    "        table_audio.insert(PrepararEmbedding(label, list_emb))\n",
    "      else:\n",
    "        print('No existen audios en el path.')\n",
    "    else:\n",
    "      print('No existe el path.')\n",
    "      \n",
    "def BusquedaAudio(audio_path):\n",
    "    print(f\"Analizando: {audio_path}\")\n",
    "    emb = GenerarEmbeddings(audio_path + '.wav')\n",
    "    busqueda = table_audio.search(vectors={'flat_index': [emb]}, n = 1)[0].iloc[0][0:2].to_list()\n",
    "    print(f\"Análisis: {busqueda}\")\n",
    "    return busqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones de Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerarAudio(hz, tiempo, i):\n",
    "    print(f\"\\nGrabando: {i}\")\n",
    "    audio = sd.rec(int(tiempo * hz), samplerate= hz, dtype='int16', channels=1)\n",
    "    sd.wait()\n",
    "    filename = f\"./demo{i}\"\n",
    "    \n",
    "    #Hilo de guardado\n",
    "    thread_guardado = threading.Thread(target = GuardarAudio, args = (filename , 1, hz, audio))\n",
    "    thread_guardado.start()\n",
    "    \n",
    "def GuardarAudio(filename, channels, hz, audio):\n",
    "    print(f\"Guardando: {filename}\")\n",
    "    with wave.open(f\"{filename}.wav\", 'wb') as wf:\n",
    "        wf.setnchannels(channels)\n",
    "        wf.setsampwidth(2) \n",
    "        wf.setframerate(hz) \n",
    "        wf.writeframes(audio.tobytes()) \n",
    "    \n",
    "    #Hilo de búsqueda de embedding\n",
    "    thread_busqueda = threading.Thread(target= BusquedaAudio, args = (filename,))\n",
    "    thread_busqueda.start()\n",
    "    \n",
    "def ComprimirAudioFLAC(filename):\n",
    "    data, samplerate = sf.read(f\"{filename}.wav\")\n",
    "    sf.write(f\"{filename}.flac\", data, samplerate, format='FLAC')\n",
    "    print(f\"Comprimido en: {filename}.flac\")\n",
    "    \n",
    "    \n",
    "    thread_eliminar = threading.Thread(target= EliminarWAV, args = (filename,))\n",
    "    thread_eliminar.start()\n",
    "     \n",
    "def EliminarWAV(filename):\n",
    "    path_wav = f\"{filename}.wav\"\n",
    "    if os.path.exists(path_wav):\n",
    "        os.remove(path_wav)\n",
    "        print(f\"Eliminando: {path_wav}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grabando: 0\n",
      "Guardando: ./demo0\n",
      "Analizando: ./demo0\n",
      "\n",
      "Grabando: 1\n",
      "Análisis: [0.7457628, 'Seba']\n",
      "Guardando: ./demo1\n",
      "Analizando: ./demo1\n",
      "\n",
      "Grabando: 2\n",
      "Análisis: [0.8044289, 'Seba']\n",
      "Guardando: ./demo2\n",
      "Analizando: ./demo2\n",
      "\n",
      "Grabando: 3\n",
      "Análisis: [0.5695438, 'Seba']\n",
      "Guardando: ./demo3\n",
      "Analizando: ./demo3\n",
      "\n",
      "Grabando: 4\n",
      "Análisis: [0.55974305, 'Seba']\n",
      "Guardando: ./demo4\n",
      "Analizando: ./demo4\n",
      "\n",
      "Grabando: 5\n",
      "Análisis: [0.7768005, 'Diego']\n",
      "Guardando: ./demo5\n",
      "Analizando: ./demo5\n",
      "\n",
      "Grabando: 6\n",
      "Análisis: [0.7932022, 'Diego']\n",
      "Guardando: ./demo6\n",
      "Analizando: ./demo6\n",
      "\n",
      "Grabando: 7\n",
      "Análisis: [0.44745898, 'Seba']\n",
      "Guardando: ./demo7\n",
      "Analizando: ./demo7\n",
      "\n",
      "Grabando: 8\n",
      "Análisis: [0.67809576, 'Seba']\n",
      "Guardando: ./demo8\n",
      "Analizando: ./demo8\n",
      "\n",
      "Grabando: 9\n",
      "Análisis: [0.681122, 'Diego']\n",
      "Guardando: ./demo9\n",
      "Analizando: ./demo9\n",
      "\n",
      "Grabando: 10\n",
      "Análisis: [0.8680649, 'Diego']\n",
      "Guardando: ./demo10\n",
      "Analizando: ./demo10\n",
      "\n",
      "Grabando: 11\n",
      "Análisis: [0.80227625, 'Diego']\n",
      "Guardando: ./demo11\n",
      "Analizando: ./demo11\n",
      "\n",
      "Grabando: 12\n",
      "Análisis: [0.77814966, 'Seba']\n",
      "Guardando: ./demo12\n",
      "Analizando: ./demo12\n",
      "\n",
      "Grabando: 13\n",
      "Análisis: [0.62169516, 'Diego']\n",
      "Guardando: ./demo13\n",
      "Analizando: ./demo13\n",
      "\n",
      "Grabando: 14\n",
      "Análisis: [0.85533965, 'Diego']\n",
      "Guardando: ./demo14\n",
      "Analizando: ./demo14\n",
      "\n",
      "Grabando: 15\n",
      "Análisis: [0.69112074, 'Diego']\n",
      "Guardando: ./demo15\n",
      "Analizando: ./demo15\n",
      "\n",
      "Grabando: 16\n",
      "Análisis: [0.61347103, 'Seba']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      8\u001b[0m     os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mGenerarAudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhz\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtiempo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m, in \u001b[0;36mGenerarAudio\u001b[1;34m(hz, tiempo, i)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGrabando: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m audio \u001b[38;5;241m=\u001b[39m sd\u001b[38;5;241m.\u001b[39mrec(\u001b[38;5;28mint\u001b[39m(tiempo \u001b[38;5;241m*\u001b[39m hz), samplerate\u001b[38;5;241m=\u001b[39m hz, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./demo\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Hilo de guardado\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ssseb\\.conda\\envs\\Prueba\\lib\\site-packages\\sounddevice.py:398\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(ignore_errors)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    395\u001b[0m \n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _last_callback:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_last_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ssseb\\.conda\\envs\\Prueba\\lib\\site-packages\\sounddevice.py:2645\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[1;34m(self, ignore_errors)\u001b[0m\n\u001b[0;32m   2639\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[0;32m   2640\u001b[0m \n\u001b[0;32m   2641\u001b[0m \u001b[38;5;124;03mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[0;32m   2642\u001b[0m \n\u001b[0;32m   2643\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2644\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2645\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2646\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[1;32mc:\\Users\\ssseb\\.conda\\envs\\Prueba\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\ssseb\\.conda\\envs\\Prueba\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 2\n",
    "hz = 44100\n",
    "tiempo = 4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "for i in range(100):\n",
    "    os.system('cls')\n",
    "    GenerarAudio(hz,tiempo, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "InsertarAudios(\"E:\\Prueba\\Seba\", 'Seba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diego</td>\n",
       "      <td>[-0.06433546, 0.3962934, -0.002036959, -0.1009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diego</td>\n",
       "      <td>[0.06982847, 0.35479352, 0.14583497, -0.019050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Diego</td>\n",
       "      <td>[0.17226578, -0.021067247, -0.08030585, -0.225...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Seba</td>\n",
       "      <td>[-0.5066348, 0.7424317, 0.5863964, -0.3717103,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seba</td>\n",
       "      <td>[0.047204513, 0.038371634, 0.15980327, -0.2737...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Seba</td>\n",
       "      <td>[0.20582007, 0.1990518, -0.34196186, -0.122000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Seba</td>\n",
       "      <td>[-0.5127218, 0.97185636, -0.29173413, -0.57133...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                         embeddings\n",
       "0  Diego  [-0.06433546, 0.3962934, -0.002036959, -0.1009...\n",
       "1  Diego  [0.06982847, 0.35479352, 0.14583497, -0.019050...\n",
       "2  Diego  [0.17226578, -0.021067247, -0.08030585, -0.225...\n",
       "3   Seba  [-0.5066348, 0.7424317, 0.5863964, -0.3717103,...\n",
       "4   Seba  [0.047204513, 0.038371634, 0.15980327, -0.2737...\n",
       "5   Seba  [0.20582007, 0.1990518, -0.34196186, -0.122000...\n",
       "6   Seba  [-0.5127218, 0.97185636, -0.29173413, -0.57133..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_audio.query()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Prueba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
